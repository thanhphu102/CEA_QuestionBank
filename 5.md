Question 1: What is the primary purpose of cache memory in a computer system?
A. To serve as the main storage for all system data.
B. To provide a slower, but larger, storage alternative to main memory.
C. To reduce the average time to access data from the main memory.
D. To directly connect to input/output devices for data transfer.

Correct answer is: C. To reduce the average time to access data from the main memory.
Because: Cache memory is designed to be faster than main memory, and its primary function is to store frequently accessed data so that the CPU can retrieve it more quickly, thereby reducing the average memory access time.

Question 2: In cache memory principles, what is defined as the minimum unit of transfer between cache and main memory?
A. Line 
B. Frame 
C. Tag 
D. Block 

Correct answer is: D. Block 
Because: The document states that a "Block" is the minimum unit of transfer between cache and main memory.

Question 3: What is a "Line" in the context of cache memory?
A. A logical address used for memory management.
B. A portion of cache memory capable of holding one block. 
C. The total capacity of the main memory.
D. A physical connection between the CPU and main memory.

Correct answer is: B. A portion of cache memory capable of holding one block. 
Because: A "Line" is defined as a portion of cache memory capable of holding one block.

Question 4: Which of the following is NOT listed as an element of cache design in the provided material?
A. Cache Size 
B. Write Policy 
C. Processor Speed
D. Replacement Algorithm 

Correct answer is: C. Processor Speed
Because: Table 5.1, "Elements of Cache Design," lists Cache Addresses, Cache Size, Write Policy, Mapping Function, Replacement Algorithm, Line Size, and Number of Caches. Processor Speed is not explicitly listed as an element of cache design.

Question 5: What is "Virtual memory" as described in the document?
A. Physical memory directly accessible by the CPU.
B. A facility that allows programs to address memory from a logical point of view. 
C. A type of cache used for very fast data access.
D. Memory specifically designed for I/O operations.

Correct answer is: B. A facility that allows programs to address memory from a logical point of view. 
Because: Virtual memory is described as a "Facility that allows programs to address memory from a logical point of view, without regard to the amount of main memory physically available".

Question 6: In a system utilizing virtual memory, what component translates each virtual address into a physical address in main memory?
A. Cache Controller
B. System Bus
C. Memory Management Unit (MMU) 
D. Processor Register

Correct answer is: C. Memory Management Unit (MMU) 
Because: The document states that when virtual memory is used, a "hardware memory management unit (MMU) translates each virtual address into a physical address in main memory".

Question 7: According to the document, what is one of the motivations for minimizing cache size?
A. To increase the overall average access time.
B. Larger caches are slightly slower than small ones due to more gates involved in addressing. 
C. To reduce the hit ratio of the cache.
D. To make the overall average cost per bit closer to that of the cache alone.

Correct answer is: B. Larger caches are slightly slower than small ones due to more gates involved in addressing. 
Because: The document states that "The larger the cache, the larger the number of gates involved in addressing the cache resulting in large caches being slightly slower than small ones".

Question 8: Which cache mapping method allows each block of main memory to map to one unique line of cache, where the line portion of the address is used to access the cache line and the tag portion is used to check for a hit?
A. Fully Associative 
B. Set Associative 
C. Direct Mapped 
D. Content-Addressable Memory



Correct answer is: C. Direct Mapped 
Because: Table 5.3 describes Direct Mapped cache as having "Each block of main memory maps to one unique line of cache," and the "Line portion of address used to access cache line; Tag portion used to check for hit on that line".


Question 9: In which cache mapping method can each block of main memory map to any line of cache, and the tag portion of the address is used to check every line for a hit?
A. Direct Mapped 
B. Fully Associative 
C. Set Associative 
D. Way Prediction



Correct answer is: B. Fully Associative 
Because: Table 5.3 states that in "Fully Associative" mapping, "Each block of main memory can map to any line of cache" and the "Tag portion of address used to check every line for hit on that line".


Question 10: What is a key characteristic of Content-Addressable Memory (CAM) as described?
A. It is less expensive than regular SRAM chips.
B. It searches its entire memory in parallel for a match when a bit string is supplied. 
C. It requires multiple clock cycles to find a match.
D. It holds more data than regular SRAM chips.

Correct answer is: B. It searches its entire memory in parallel for a match when a bit string is supplied. 
Because: The document states that "A CAM is designed such that when a bit string is supplied, the CAM searches its entire memory in parallel for a match".

Question 11: Which cache replacement algorithm is described as the "most effective" and "most popular" due to its simplicity of implementation, replacing the block in the set that has been in the cache longest with no reference to it?
A. First-in-first-out (FIFO) 
B. Least frequently used (LFU) 
C. Random
D. Least recently used (LRU) 


Correct answer is: D. Least recently used (LRU) 
Because: LRU is described as the "Most effective" and "most popular replacement algorithm" due to its simplicity, replacing the block in the set that has been in the cache longest with no reference to it.

Question 12: When a block in the cache has been altered, and it is to be replaced, what action must occur before bringing in a new block, according to the write policy discussion?
A. The old block can be overwritten directly without any prior action.
B. The old block is moved to a separate buffer.
C. Main memory must be updated by writing the line of cache out to the block of memory. 
D. The cache simply discards the old block.

Correct answer is: C. Main memory must be updated by writing the line of cache out to the block of memory. 
Because: The document states: "If at least one write operation has been performed on a word in that line of the cache then main memory must be updated by writing the line of cache out to the block of memory before bringing in the new block".

Question 13: What is the main disadvantage of the "Write through" write policy?
A. It is complex to implement.
B. It minimizes memory writes.
C. It generates substantial memory traffic and may create a bottleneck. 
D. It makes portions of main memory invalid.

Correct answer is: C. It generates substantial memory traffic and may create a bottleneck. 
Because: The document states that "The main disadvantage of this technique is that it generates substantial memory traffic and may create a bottleneck".

Question 14: Which write miss alternative involves fetching the block containing the word to be written from main memory (or next level cache) into the cache, and the processor proceeds with the write cycle?
A. No write allocate 
B. Write back
C. Write through
D. Write allocate 

Correct answer is: D. Write allocate 
Because: "Write allocate" is defined as the alternative where "The block containing the word to be written is fetched from main memory (or next level cache) into the cache and the processor proceeds with the write cycle".

Question 15: What problem is introduced in a bus organization where more than one device has a cache and main memory is shared, especially if data in one cache are altered?
A. Bus congestion
B. Memory fragmentation
C. Cache coherency 
D. Processor overheating

Correct answer is: C. Cache coherency 
Because: The document introduces "Cache Coherency" as the new problem in such a scenario, where "If data in one cache are altered, this invalidates not only the corresponding word in main memory, but also that same word in other caches"[cite: 118].

Question 16: As block size increases in a cache, what is the initial effect on the hit ratio, and what happens eventually?
A. The hit ratio decreases at first, then increases.
B. The hit ratio remains constant.
C. The hit ratio will at first increase, then begin to decrease. 
D. The hit ratio continuously increases.

Correct answer is: C. The hit ratio will at first increase, then begin to decrease. 
Because: The document explains that "As the block size increases the hit ratio will at first increase because of the principle of locality" but then "The hit ratio will begin to decrease as the block becomes bigger and the probability of using the newly fetched information becomes less than the probability of reusing the information that has to be replaced"[cite: 121, 122].

Question 17: What is an advantage of a split cache design (one dedicated to instructions, one to data)?
A. Higher hit rate 
B. Simplifies searching for data with multiple processors.
C. Eliminates cache contention between instruction fetch/decode unit and execution unit. 
D. Only one cache needs to be designed and implemented.


Correct answer is: C. Eliminates cache contention between instruction fetch/decode unit and execution unit. 
Because: The advantage of split caches is that they "Eliminates cache contention between instruction fetch/decode unit and execution unit," which is "Important in pipelining".

Question 18: Consider a cache timing model equation for a Direct-Mapped cache: thit = trl + txb. If trl (read latency) is 5ns and txb (transfer block time) is 15ns, what is the thit (hit time)?
A. 5ns
B. 15ns
C. 20ns
D. 10ns

Correct answer is: C. 20ns
Because: The formula for thit in a Direct-Mapped cache is thit = trl + txb.
Given trl = 5ns and txb = 15ns,
thit = 5ns + 15ns = 20ns.

Question 19: For a Fully Associative cache, the miss time is simply the tag comparison time (tct). If tct is 8ns, what is the tmiss (miss time) for a Fully Associative cache?
A. 0ns
B. 8ns
C. 16ns
D. 4ns

Correct answer is: B. 8ns
Because: The document states that for a Fully Associative cache, "the miss time is simply the tag comparison time" (tct). Therefore, if tct = 8ns, tmiss = 8ns.

Question 20: Which of the following is a technique for cache performance improvement mentioned in Table 5.7?
A. Decreasing the degree of associativity. 
B. Using a unified cache. 
C. Implementing Way Prediction. 
D. Reducing the line size. 

Correct answer is: C. Implementing Way Prediction. 
Because: Table 5.7, "Cache Performance Improvement Techniques," lists "Way Prediction" as a technique to reduce tmiss. Other options like "Decreasing the degree of associativity" and "Reducing the line size" are listed under techniques that reduce cache performance, while "Unified cache" is a type of cache organization, not an improvement technique in the context of the table.